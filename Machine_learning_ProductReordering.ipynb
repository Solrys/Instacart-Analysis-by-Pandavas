{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import hvplot.pandas\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas to execute SQL queries\n",
    "# Import dependencies\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2\n",
    "\n",
    "#Create a connection to the RDS instance\n",
    "connection = psycopg2.connect(\n",
    "    host = 'instacart-db.crrysho2rjsv.us-east-2.rds.amazonaws.com',\n",
    "    port = 5432,\n",
    "    user = 'postgres',\n",
    "    password = 'G3DBpsW0rd',\n",
    "    database='instacart'\n",
    "    )\n",
    "cursor=connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Datasets \n",
    "# Read the Prior Products Orders dataset\n",
    "sql = \"select * from order_products_prior\"\n",
    "orders_prior_df = pd.read_sql(sql, con=connection)\n",
    "orders_prior_df\n",
    "\n",
    "# Read the Prior Orders dataset\n",
    "sql = \"select * from order_prior\"\n",
    "allorders_df = pd.read_sql(sql, con=connection)\n",
    "allorders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../instacart-data/products.csv\"\n",
    "products_df = pd.read_csv(file_path)\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the Orders Prior Dataset for ML\n",
    "#data = orders_prior_df.sample(2250000)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the Prior Dataset with the Main Orders Dataset to retrieve other columns\n",
    "orders_df = orders_prior_df.merge(allorders_df, how=\"inner\", on=\"order_id\")\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna(inplace=True)\n",
    "orders_df.drop_duplicates(inplace=True)\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_df.merge(products_df, how=\"left\", on=\"product_id\")\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Orders by Product table for getting number of orders per product\n",
    "sql = \"select product_id, num_of_orders from orders_by_product\"\n",
    "orders_prod_df = pd.read_sql(sql, con=connection)\n",
    "orders_prod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_prod_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top ten Reordered Products\n",
    "topten_ords_df = orders_prod_df.sort_values(ascending=False, by=\"num_of_orders\")\n",
    "topten_ords_df = topten_ords_df[:10]\n",
    "topten_ords_df['product_id'] = topten_ords_df['product_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas to execute SQL queries\n",
    "# Get the number of Reorders by product from the database table\n",
    "sql = \"select product_id, num_of_reorders from reorders_by_product\"\n",
    "reordprod_df = pd.read_sql(sql, con=connection)\n",
    "reordprod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordprod_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top ten Reordered Products\n",
    "top_reords_df = reordprod_df.sort_values(ascending=False, by=\"num_of_reorders\")\n",
    "topten_reords_df = top_reords_df[:10]\n",
    "topten_reords_df['product_id'] = topten_reords_df['product_id'].astype(str)\n",
    "#px.bar(topten_reords_df, x='product_id', y='num_of_reorders', hover_data=[\"product_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Orders dataset with Reorders dataset\n",
    "orders_df = orders_df.merge(reordprod_df, how=\"left\", on=\"product_id\")\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Orders dataset with Orders by product dataset\n",
    "orders_df = orders_df.merge(orders_prod_df, how=\"left\", on=\"product_id\")\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the Num_Orders and Num_Reorders column with 0 if empty\n",
    "orders_df[\"num_of_orders\"] = orders_df[\"num_of_orders\"].fillna(0)\n",
    "orders_df[\"num_of_reorders\"] = orders_df[\"num_of_reorders\"].fillna(0)\n",
    "orders_df.dropna()\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Department Dataset\n",
    "file_path = \"../instacart-data/departments.csv\"\n",
    "dept_df = pd.read_csv(file_path)\n",
    "dept_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Department Dataset\n",
    "file_path = \"../instacart-data/aisles.csv\"\n",
    "aisles_df = pd.read_csv(file_path)\n",
    "aisles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with department and aisle dataset\n",
    "orders_df = orders_df.merge(dept_df, how=\"left\", on=\"department_id\")\n",
    "orders_df = orders_df.merge(aisles_df, how=\"left\", on=\"aisle_id\")\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Dataset for ProductsClustering\n",
    "orders_df.to_csv(\"OrdersProducts_Consolidated.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 1 - Drop the columns that may not contribute to ML\n",
    "orders_df.drop(columns=[\"product_id\", \"aisle_id\", \"department_id\", 'user_id', \"order_number\", \"eval_set\", \"order_id\", \"product_name\"], inplace=True)\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include data points where num_of_reords is greater than 1\n",
    "orders_df = orders_df.loc[orders_df.num_of_reorders > 1]\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include data points where num_of_orders is greater than 5\n",
    "orders_df = orders_df.loc[orders_df.num_of_orders > 5]\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.get_dummies(orders_df, columns=[\"department\", \"aisle\", \"order_dow\", \"order_hour_of_day\"])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop_duplicates(inplace=True)\n",
    "#final_df.to_csv(\"Ordersanalysis.csv\")\n",
    "#final_df.drop(columns=\"days_since_prior_order\", inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Final Dataset into \"Target\" and \"Features\"\n",
    "X = final_df.drop(\"reordered\", axis=1)\n",
    "y = pd.DataFrame(final_df[\"reordered\"])\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_nn_model(input_features:int, layer1:int, layer2:int, add_layers:int, \n",
    "                   actv_fun1:str, actv_func2:str, output_func:str,)->float:\n",
    "\n",
    "    # Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "    number_input_features = input_features\n",
    "    hidden_nodes_layer1 = layer1\n",
    "    hidden_nodes_layer2 = layer2\n",
    "\n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features ,activation=actv_fun1))\n",
    "\n",
    "    # Second hidden layer\n",
    "    nn.add(\n",
    "        tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=actv_func2 )\n",
    "    )\n",
    "    while (add_layers != 0):\n",
    "        nn.add(\n",
    "        tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=actv_func2 )\n",
    "        )\n",
    "        add_layers = add_layers - 1\n",
    "        \n",
    "    # Output layer\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation=output_func))\n",
    "    \n",
    "    return nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Training Function for the Neural Network\n",
    "def train_nn(nn:float, num_epochs:int):\n",
    "    # Import checkpoint dependencies\n",
    "    import os\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    # Define the checkpoint path and filenames\n",
    "    os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "    checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\"\n",
    "\n",
    "    # Create a callback that saves the model's weights every 5 epochs\n",
    "    cp_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_freq=50000)\n",
    "    \n",
    "    # Compile the model\n",
    "    nn.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Train the model\n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=num_epochs, callbacks=[cp_callback])\n",
    "    \n",
    "    # Evaluate the model using the test data\n",
    "    model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "    \n",
    "    # Export our model to HDF5 file\n",
    "    nn.save(\"ProdReordering.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 1\n",
    "\n",
    "inputs = len(X_train_scaled[0])\n",
    "layer1_nodes = 3 * inputs\n",
    "layer2_nodes = 2 * inputs             \n",
    "\n",
    "# Attempt 1 - \n",
    "nn = setup_nn_model(inputs, layer1_nodes, layer2_nodes, 1, \"relu\", \"relu\", \"sigmoid\")\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()\n",
    "\n",
    "#Train the NN\n",
    "train_nn(nn, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Attempts here to follow -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 2 - Drop columns\n",
    "final_df = final_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "log_classifier = LogisticRegression(solver=\"lbfgs\",max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "log_classifier.fit(X_train_scaled,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = log_classifier.predict(X_test_scaled)\n",
    "print(f\" Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create the SVM model\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "print(f\" SVM model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
